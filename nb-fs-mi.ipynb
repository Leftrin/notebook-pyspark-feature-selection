{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection using Pearson correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ is a collection of datasets for classification and regression. We will use some of them to test our feature selection algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "filename = \"german.numer_scale\"\n",
    "url = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/\" + filename\n",
    "f = urllib.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_MLlib_ relies on _LabeledPoint_ as data structure to that stores a numerical vector (dense or sparse) and a numerical label. An RDD of LabeledPoint represents the dataset given as input to train or test supervised machine learning models.\n",
    "\n",
    "Spark provides a built-in function to tranforms a libsvm dataset into a RDD[LabeledPoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "rdd = MLUtils.loadLibSVMFile(sc, filename)\n",
    "ncols = rdd.first().features.size  # number of columns (no class) of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we create first the Pearson correlation coefficients (PCCs) between the class and each features (_scoreClass_), then the PCCs between every pair of feature (_scoreMatrix_). Once these intermediate results are completed, we proceed into performing the feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "def meltLPclass(lp):\n",
    "    '''\n",
    "    This function creates a list of k,v tuples, one per each\n",
    "    label-feature combination. 'k' corresponds to the index\n",
    "    of the feature and 'v' corresponds to a tuple of two\n",
    "    elements: value of the label, value of the feature\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lp : LabeledPoint\n",
    "        a point in the feature space with label\n",
    "    '''\n",
    "    label = lp.label\n",
    "    features = lp.features\n",
    "    r = range(features.size)\n",
    "    return [(i, (label, features[i])) for i in r]\n",
    "\n",
    "def meltLPfeatures(lp):\n",
    "    '''\n",
    "    This function creates a list of k,v tuples, one per each\n",
    "    feature-feature combination. 'k' corresponds to the index\n",
    "    of the features and 'v' corresponds to the values of the\n",
    "    features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lp : LabeledPoint\n",
    "        a point in the feature space with label\n",
    "    '''\n",
    "    label = lp.label\n",
    "    features = lp.features\n",
    "    r = range(features.size)\n",
    "    return [((i, j), (features[i], features[j])) for i in r for j in r if i < j]\n",
    "\n",
    "def corr(x):\n",
    "    '''\n",
    "    This function calculates the Pearson correlation coefficient\n",
    "    among two variables. It returns the index of the feature and\n",
    "    its correlation coefficient\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tuple\n",
    "        x[0] is a scalar value (or a tuple), representing the index(es) of the feature(s)\n",
    "        x[1] is a pyspark.resultiterable.ResultIterable object\n",
    "    '''\n",
    "    idx = x[0]\n",
    "    values = list(x[1])\n",
    "    \n",
    "    l = list(values)\n",
    "    v1, v2 = zip(*values)\n",
    "    p = pearsonr(v1, v2)[0]\n",
    "    \n",
    "    return (idx, p)\n",
    "\n",
    "def createScoreMatrix(pairs, ncols):\n",
    "    '''\n",
    "    This functions builds a NxN matrix, where N = ncols.\n",
    "    Rows and columns corresponds to the indexes of features\n",
    "    and values are the computed scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : list\n",
    "        each element of the list is a k,v tuple,\n",
    "        'k' is a tuple (a, b), where 'a' and 'b' are both feature indexes\n",
    "        'v' is the score value\n",
    "    ncols : int\n",
    "        number of features (no class) in the dataset\n",
    "    '''\n",
    "    scoreMatrix = np.zeros((ncols, ncols))\n",
    "    \n",
    "    for i in range(len(pairs)):\n",
    "        t = pairs[i]\n",
    "        row = t[0][0]\n",
    "        col = t[0][1]\n",
    "        v = t[1]\n",
    "    \n",
    "    scoreMatrix[row][col] = v\n",
    "    \n",
    "    return scoreMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fscores = rdd.flatMap(meltLPclass).groupByKey().map(corr).collect()\n",
    "fscoresIdx, fscoresScore = zip(*fscores)\n",
    "scoreClass = [fscoresScore[fscoresIdx.index(i)] for i in range(ncols)]\n",
    "\n",
    "fpairs = rdd.flatMap(meltLPfeatures).groupByKey().map(corr).collect()\n",
    "scoreMatrix = createScoreMatrix(fpairs, ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the scores label-feature and feature-feature, we select the top _nfs_ features that best correlate with the label. _fsIdx_ stores the indexes of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfs = 5  # number of feature to select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = zip([abs(x) for x in scoreClass], range(len(scoreClass)))\n",
    "df.sort(key=lambda tup: tup[0], reverse=True)\n",
    "fsIdx = [x[1] for x in df[0:nfs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def reduceLP(lp, fsIdx):\n",
    "    label = lp.label\n",
    "    features = lp.features\n",
    "    v = [features[i] for i in fsIdx]\n",
    "    return LabeledPoint(label, Vectors.dense(v))\n",
    "\n",
    "rddFS = rdd.map(lambda x: reduceLP(x, fsIdx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
